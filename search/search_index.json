{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Infrastructure Documentation","text":"<p>This documentation provides an in-depth look at our project's infrastructure design and implementation. It details my thought process, design decisions, and problem-solving approaches. Whether you're a developer, network architect, or end-user, you'll gain insights into how this system evolved, including:</p> <ul> <li>sources used</li> <li>challenges overcome</li> <li>lessons learned along the way</li> </ul>"},{"location":"#high-level-design","title":"High-Level Design","text":"<p>Here's an overview of the infrastructure:</p> <p></p> <p>Infrastructure Design</p>"},{"location":"#technology-used","title":"Technology Used","text":"<ul> <li> Flask/Python</li> <li> <p>Flask, a lightweight Python web framework, enables rapid development of scalable web applications.</p> </li> <li> <p> Docker</p> </li> <li> <p>Docker simplifies containerization, ensuring consistent environments across development and production.</p> </li> <li> <p> HAProxy</p> </li> <li> <p>HAProxy, a reliable open-source load balancer, optimizes traffic distribution and enhances scalability.</p> </li> <li> <p> Nagios</p> </li> <li>Nagios offers powerful monitoring capabilities, ensuring system health and uptime with detailed alerts.</li> </ul>"},{"location":"#documentation-overview","title":"Documentation Overview","text":"<p>This documentation includes the following sections:</p> <ul> <li>Project Overview: (This page) High-level overview of the system and its architecture.</li> <li>About: Background and additional details about the project.</li> </ul>"},{"location":"#get-started","title":"Get Started","text":"<p>To dive deeper, visit the Getting Started section or explore About.</p> <p>Visit the Github Repo</p>"},{"location":"about/","title":"About","text":"<p>Welcome to the \"About\" section of the Infrastructure Documentation. This page provides background information about the project, including its purpose, goals, and the context in which it was developed.</p>"},{"location":"about/#project-purpose-and-objectives","title":"Project Purpose and Objectives","text":"<p>The primary goal of this project is to:</p> <ul> <li>Create Two Web Servers</li> <li>Create a LB that will serve the users</li> <li>Create a Nagio Server to monitor all servers</li> <li>Lockdown the system to limited ports<ul> <li>HTTP for LB only</li> <li>SSH for Nagios Only<ul> <li>Nagios having SSH capability to all other Servers.</li> </ul> </li> </ul> </li> </ul>"},{"location":"about/#key-features","title":"Key Features","text":"<p>This system includes the following highlights:</p> <ul> <li>Scalability: Designed to grow with the organization's needs.<ul> <li>Additional Services can be introduced for additional Web Servers if Needed.</li> </ul> </li> <li>Reliability: Built with robust principles to ensure uptime and performance.<ul> <li>One can add additional(reduncant LB to the mix)</li> </ul> </li> <li>Simplicity: Easy to deploy, maintain, and scale.<ul> <li>Local Development via Docker for ease of development.</li> </ul> </li> </ul>"},{"location":"about/#team-and-contributions","title":"Team and Contributions","text":"<p>This project was developed by:</p> <ul> <li>Kevin Campos: Automation Engineer</li> <li>Visit Porfolio for more information about me.</li> </ul>"},{"location":"about/#acknowledgments","title":"Acknowledgments","text":"<p>We would like to thank the following tools and frameworks that made this project possible:</p> <ul> <li>Python: The core programming language used to build and manage the project.</li> <li>Docker: Streamlined containerization for deployment and testing.</li> <li>Flask: Lightweight web framework for creating and managing web components.</li> <li>python-dotenv: Environment variable management for configuration.</li> <li>mkdocs-material: Elegant and feature-rich theme for project documentation.</li> </ul> <p>These tools were instrumental in the development, deployment, and documentation of the project.</p>"},{"location":"about/#learn-more","title":"Learn More","text":"<p>To explore more about the project, visit:</p> <ul> <li>Infrastructure Overview</li> </ul>"},{"location":"docker/","title":"Dockerized Services","text":"<p>This section explains the Dockerized architecture of the project, covering the configuration of various services, their interactions, and decisions made along the way. Below, you'll find detailed insights into the tools, technologies, and configurations used to build a robust and scalable Dockerized environment.</p>"},{"location":"docker/#overview","title":"Overview","text":"What You'll Learn <p>By the end of this section, you will understand:</p> <ul> <li>How the project's services are containerized using Docker.</li> <li>The purpose and configuration of each service, including HAProxy, web servers, and Nagios.</li> <li>Key challenges faced during the setup and how they were overcome.</li> <li>Lessons learned and future improvements for scaling and optimization.</li> </ul> <p>This project relies on Docker to create isolated and reproducible environments for running its services. The use of Docker Compose ensures seamless coordination and deployment.</p>"},{"location":"docker/#dockerized-setup","title":"Dockerized Setup","text":"<p>The following services are defined in the <code>docker-compose.yml</code> file:</p>"},{"location":"docker/#haproxy","title":"HAProxy","text":"<ul> <li>Image: Built from the <code>Dockerfile.dev</code> in <code>./infrastructure/load_balancer</code>.</li> <li>Purpose: Acts as a load balancer to distribute traffic between the web servers.</li> <li>Ports: Maps the port range <code>60000-60001</code> to the host for testing purposes.</li> <li>Dependencies: Depends on <code>web-a</code> and <code>web-b</code> services to ensure they start first.</li> </ul>"},{"location":"docker/#web-servers","title":"Web Servers","text":"<ul> <li>Web-A: A simple web server configured to respond with the message \"Web Server: A\".<ul> <li>Image: Built from <code>Dockerfile.dev</code> in <code>./infrastructure/web_server</code>.</li> <li>Environment: Injects the environment variable <code>MESSAGE=\"Web Server: A\"</code>.</li> <li>Ports: Maps container port <code>5001</code> to host port <code>5001</code>.</li> </ul> </li> <li>Web-B: Similar setup as Web-A, but configured to respond with \"Web Server: B\".<ul> <li>Environment: Sets <code>MESSAGE=\"Web Server: B\"</code>.</li> <li>Ports: Maps container port <code>5002</code> to host port <code>5002</code>.</li> </ul> </li> </ul>"},{"location":"docker/#nagios","title":"Nagios","text":"<ul> <li>Image: Uses the official <code>jasonrivers/nagios:latest</code> image for monitoring.</li> <li>Purpose: Provides a monitoring interface to observe the health and performance of services.</li> <li>Configuration:<ul> <li>Mounts configuration files and scripts into the container:<ul> <li><code>/opt/nagios/etc</code>: Maps the <code>etc</code> directory for Nagios configurations.</li> <li><code>/opt/nagios/libexec</code>: Maps the <code>libexec</code> directory for custom plugins.</li> </ul> </li> <li>Ports: Exposes Nagios on host port <code>8080</code>.</li> <li>Environment: Sets <code>NAGIOS_TIMEZONE=UTC</code> for consistent timestamps.</li> <li>Restart Policy: Always restarts to ensure availability.</li> </ul> </li> </ul>"},{"location":"docker/#mkdocs","title":"MkDocs","text":"<ul> <li>Image: Uses the official <code>squidfunk/mkdocs-material</code> image.</li> <li>Purpose: Serves project documentation locally during development.</li> <li>Configuration:<ul> <li>Maps the local <code>mkdocs</code> directory into the container for live edits.</li> <li>Ports: Serves documentation on host port <code>8000</code>.</li> </ul> </li> </ul>"},{"location":"docker/#network","title":"Network","text":"<ul> <li>Name: <code>expensify_network</code> (custom bridge network).</li> <li>Purpose: Ensures seamless communication between all containers.</li> </ul>"},{"location":"docker/#challenges-faced","title":"Challenges Faced","text":"Challenges Encountered <ul> <li>Dependency Management: Ensuring all services start in the correct order.</li> <li>Port Conflicts: Avoiding collisions when mapping multiple container ports to the host.</li> <li>Configuration Complexity: Managing multiple configuration files and ensuring they work seamlessly.</li> </ul> <p>How I Solved Them</p> <ul> <li>Dependency Management: Used the <code>depends_on</code> key in <code>docker-compose.yml</code> to specify service dependencies.</li> <li>Port Conflicts: Assigned unique ports for each service and documented mappings clearly.</li> <li>Configuration Complexity: Organized configuration files into well-defined directories for easier maintenance.</li> </ul>"},{"location":"docker/#lessons-learned","title":"Lessons Learned","text":"Key Takeaways <ul> <li>Docker Compose simplifies service orchestration but requires clear documentation for maintainability.</li> <li>Using environment variables allows dynamic configurations and easier scaling.</li> <li>Consistent naming conventions and well-structured directories significantly improve project clarity.</li> </ul> <p>Future Enhancements</p> <ul> <li>Scaling: Investigate Docker Swarm or Kubernetes for scaling the architecture to handle higher traffic.</li> <li>Monitoring: Enhance monitoring with Prometheus and Grafana for detailed insights.</li> <li>Security: Implement SSL/TLS termination at HAProxy for secure communication.</li> </ul>"},{"location":"getting_started/","title":"Getting Started","text":"<p>This guide provides detailed steps to set up and start the project using the provided <code>tasks.py</code> file and Poetry for dependency management. Follow these instructions to get up and running quickly.</p>"},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following installed on your system:</p>"},{"location":"getting_started/#installing-prerequisites","title":"Installing Prerequisites","text":"<ul> <li>Python 3.10 or later: Download and install Python</li> <li>Docker and Docker Compose: Install Docker</li> <li>Poetry: Install Poetry</li> <li>MkDocs: Included in Poetry dependencies; no separate installation required.</li> </ul>"},{"location":"getting_started/#setting-up-the-environment","title":"Setting Up the Environment","text":"<pre><code>graph TD\n    A[Clone Repository] --&gt; B[Initialize Poetry]\n    B --&gt; C[Create ENV Files]\n    C --&gt; D[Invoke Tasks]</code></pre>"},{"location":"getting_started/#high-level-folder-structure","title":"High-Level Folder Structure","text":"<pre><code>infrastructure/\n\u251c\u2500\u2500 load_balancer/\n\u2502   \u251c\u2500\u2500 Dockerfile.dev\n\u2502   \u251c\u2500\u2500 haproxy.cfg\n\u251c\u2500\u2500 web_server/\n\u2502   \u251c\u2500\u2500 Dockerfile.dev\n\u2502   \u251c\u2500\u2500 app.py\n\u251c\u2500\u2500 nagios/\n\u2502   \u251c\u2500\u2500 etc/\n\u2502   \u251c\u2500\u2500 libexec/\nmkdocs/\n\u251c\u2500\u2500 docs/\n\u251c\u2500\u2500 mkdocs.yml\ndocker-compose*.yml\npyproject.yml\npoetry.lock\ntasks.py\n</code></pre>"},{"location":"getting_started/#steps-to-start-docker-containers","title":"Steps to Start Docker Containers","text":""},{"location":"getting_started/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/kvncampos/infrastructureSetup\n</code></pre>"},{"location":"getting_started/#2-poetry-init","title":"2. Poetry Init","text":"<pre><code>poetry shell\npoetry install\n</code></pre>"},{"location":"getting_started/#3-create-env-files","title":"3. Create ENV Files","text":"<p>At the root level: <pre><code>touch .env\n\nPROJECT_NAME=&lt;projectName&gt;\nENV=development\n</code></pre> At infrastructure/nagios/: <pre><code>touch .env\n\nNAGIOSADMIN_USER=nagiosadmin\nNAGIOSADMIN_PASSWORD=&lt;Passoword&gt;\nNAGIOS_TIMEZONE=UTC\n</code></pre></p>"},{"location":"getting_started/#4-invoke-tasks","title":"4. Invoke Tasks","text":"<pre><code>invoke up\n</code></pre> Quick Access to Containers <ul> <li>MKDocs</li> <li>Web Server A</li> <li>Web Server B</li> <li>HAProxy LB</li> <li>Nagios</li> </ul>"},{"location":"getting_started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting_started/#docker-compose-fails-to-start","title":"Docker Compose Fails to Start","text":"<ul> <li>Issue: Error: <code>Cannot connect to the Docker daemon</code>.</li> <li>Solution: Ensure Docker is running: <code>sudo systemctl start docker</code>.</li> </ul>"},{"location":"getting_started/#poetry-command-not-found","title":"Poetry Command Not Found","text":"<ul> <li>Issue: Poetry commands are not recognized.</li> <li>Solution: Add Poetry to your PATH: <code>export PATH=\"$HOME/.poetry/bin:$PATH\"</code>.</li> </ul>"},{"location":"lessons_learned/","title":"Lessons Learned: Network Configuration Issues","text":"<p>This document outlines lessons learned from two critical issues encountered during the deployment and configuration of a Dockerized HAProxy load balancer and associated services. These insights aim to help avoid similar mistakes in future projects.</p>"},{"location":"lessons_learned/#incident-overview","title":"Incident Overview","text":"<ol> <li> <p>Accidentally Terminating SSH Without Verifying Access:     The SSH session was prematurely closed before testing firewall rules, resulting in locked-out access.</p> </li> <li> <p>Opening a Large Port Range (60000-65000):     Opening this range conflicted with Docker's networking stack, causing the server to become unresponsive and preventing SSH access. This rendered the HAProxy component non-operational.</p> </li> </ol>"},{"location":"lessons_learned/#mistake-1-accidental-ssh-disconnection","title":"Mistake 1: Accidental SSH Disconnection","text":""},{"location":"lessons_learned/#what-happened","title":"What Happened","text":"<p>The SSH session was closed before validating connectivity, preventing re-access to the server after firewall changes.</p>"},{"location":"lessons_learned/#how-to-prevent-this","title":"How to Prevent This","text":"<ol> <li> <p>Verify Port Ranges and Limitation:</p> <ul> <li>Verify the port ranges are correct and pick services that best accommodate the requirements.</li> </ul> </li> <li> <p>Validate Access Before Exiting:</p> <ul> <li>Test SSH connectivity from a separate session after applying firewall rules.</li> <li>Keep at least one SSH session active until all configurations are verified.</li> </ul> </li> <li> <p>Enable Recovery Options:</p> <ul> <li>Console Access: Ensure cloud console access (e.g., AWS EC2) is available.</li> <li>Backup SSH Access: Allow a trusted IP range:     <pre><code>sudo ufw allow from &lt;trusted_ip&gt; to any port 22\n</code></pre><ul> <li>Backup/Recovery Account</li> </ul> </li> </ul> </li> <li> <p>Automate Recovery:</p> <ul> <li>Use AWS Systems Manager or equivalent tools to ensure an out-of-band recovery mechanism.</li> </ul> </li> <li> <p>Remove Public Access Last:</p> <ul> <li>Only remove public access after verifying internal connectivity is functional.</li> </ul> </li> </ol>"},{"location":"lessons_learned/#mistake-2-opening-ports-60000-65000","title":"Mistake 2: Opening Ports 60000-65000","text":""},{"location":"lessons_learned/#what-happened_1","title":"What Happened","text":"<p>Opening the range <code>60000-65000</code> caused conflicts with Docker's networking stack, binding these ports and disrupting critical services like SSH.</p>"},{"location":"lessons_learned/#how-to-prevent-this_1","title":"How to Prevent This","text":"<ol> <li> <p>Use Minimal Port Ranges:</p> <ul> <li>Instead of large ranges, identify and open only required ports:     <pre><code>ports:\n- \"60000:60000\"\n- \"60001:60001\"\n</code></pre></li> </ul> </li> <li> <p>Pre-Test Configurations:</p> <ul> <li>Validate all changes in a local or staging environment to identify potential issues.</li> </ul> </li> <li> <p>Ensure SSH Availability:</p> <ul> <li>Monitor critical ports like <code>22</code> during testing.</li> <li>Use tools such as <code>netstat</code> to check for port conflicts:     <pre><code>sudo netstat -tuln | grep 22\n</code></pre></li> </ul> </li> <li> <p>Run HAProxy in network_mode: host:</p> <ul> <li>Using --network host vs. Port Binding</li> </ul> Feature Port Binding (-p) --network host Performance Slight NAT overhead for every request. No NAT, direct host networking. Configuration Simplicity Need to manage port mappings explicitly. No port mappings needed. Port Conflicts Only mapped ports cause conflicts. All bound ports can conflict. Use Case Fine for small ranges (e.g., 10 ports). Better for large ranges (e.g., 5000). </li> </ol>"},{"location":"lessons_learned/#comparison-iptables-vs-network_mode-host","title":"Comparison: iptables vs. <code>network_mode: host</code>","text":"<p>Using <code>network_mode: host</code> offers a simpler alternative to managing networking compared to <code>iptables</code>.</p>"},{"location":"lessons_learned/#key-differences","title":"Key Differences","text":"Feature <code>iptables</code> <code>network_mode: host</code> Complexity Requires manual configuration of rules. Simplifies setup by avoiding manual rules. Flexibility Highly customizable for advanced scenarios. Limited to exposing the container's network. Ease of Use Steeper learning curve for configuration. Straightforward and easy to implement. Performance Slight overhead for rule processing. Minimal overhead, direct host networking."},{"location":"lessons_learned/#summary","title":"Summary","text":"<ul> <li><code>iptables</code> is ideal for advanced and highly granular networking scenarios where precise control is required.</li> <li><code>network_mode: host</code> is best suited for simpler setups where performance and ease of use are the primary concerns.</li> <li> <p>By using <code>network_mode: host</code>, you can achieve the same outcomes as basic <code>iptables</code> configurations without the added complexity of managing individual rules.</p> Things to Consider <ul> <li>Compatibility: <code>network_mode: host</code> is supported only on Linux. It does not work on macOS or Windows.</li> <li>Port Conflicts: Ensure that no other services on the host are using ports in the range <code>60000-60010</code> or any ports required by your application.</li> <li>Security: Since the container shares the host's network stack, additional precautions are necessary. For example, configure firewalls to secure any exposed ports.</li> </ul> </li> </ul>"},{"location":"lessons_learned/#why-this-matters","title":"Why This Matters","text":"<ul> <li>Simplicity: Avoids the complexity of managing individual port bindings or writing complex iptables rules.</li> <li>Performance: Eliminates the slight NAT overhead that comes with -p port bindings, which is particularly beneficial for applications handling high throughput or a large number of open connections.</li> </ul>"},{"location":"lessons_learned/#general-lessons-for-future-deployments","title":"General Lessons for Future Deployments","text":"<ol> <li> <p>Plan and Test Incrementally:</p> <ul> <li>Apply changes in small increments, verifying each step before proceeding.</li> <li>Use scripts or playbooks to document and automate changes.</li> </ul> </li> <li> <p>Backup Critical Configurations:</p> <ul> <li>Save copies of firewall rules and Docker configurations before making changes.</li> </ul> </li> <li> <p>Deploy a Jump Host:</p> <ul> <li>Use a bastion host to maintain consistent access even during misconfigurations.</li> </ul> </li> <li> <p>Restrict Exposure:</p> <ul> <li>Open only the ports required for functionality and limit external access to critical resources.</li> </ul> </li> <li> <p>Monitor Changes in Real Time:</p> <ul> <li>Use logging and monitoring tools to track the impact of changes and detect issues early.</li> </ul> </li> </ol> <p>End Result</p> <p>By implementing these practices, future challenges can be handled more effectively, minimizing downtime and ensuring successful deployment.</p>"},{"location":"loadbalancer/","title":"Load Balancer: HAProxy","text":"<p>This section covers the implementation of HAProxy as the load balancer for this project, including insights into its setup, configuration, challenges faced, and lessons learned. Below, you'll find detailed explanations, relevant configurations, and external resources that guided the implementation.</p>"},{"location":"loadbalancer/#overview","title":"Overview","text":"What You'll Learn <p>By the end of this section, you will understand:</p> <ul> <li>Why HAProxy was chosen as the load balancer.</li> <li>The process of setting up and configuring HAProxy.</li> <li>Key challenges faced during the implementation.</li> <li>Lessons learned and resources that guided the implementation.</li> </ul> <p>HAProxy plays a critical role in this project by acting as a reverse proxy to distribute incoming traffic efficiently across multiple web servers. This setup enhances scalability and reliability, ensuring optimal performance under varying loads.</p>"},{"location":"loadbalancer/#choosing-haproxy","title":"Choosing HAProxy","text":""},{"location":"loadbalancer/#why-haproxy","title":"Why HAProxy?","text":"Key Benefits of HAProxy <ul> <li>Performance: Highly efficient and capable of handling high traffic volumes.</li> <li>Flexibility: Supports various load-balancing algorithms and configurations.</li> <li>Stability: Proven reliability in production environments for years.</li> <li>Open Source: Community-driven with strong documentation and support.</li> </ul> <p>While evaluating frameworks, I considered:</p> <ul> <li>HAProxy: Known for performance and configurability in handling web traffic.lity.</li> <li>Nginx: A versatile choice, but I preferred a dedicated load balancer for this project.</li> <li>Traefik: Ideal for containerized environments but lacked some features I required.</li> </ul> <pre><code>graph TD\n    A[Load Balancer Evaluation] --&gt;|Performance| B(HAProxy)\n    A --&gt;|Flexibility| C(Nginx)\n    A --&gt;|Container Integration| D(Traefik)\n    B --&gt; E[Final Choice]</code></pre> <p>After comparing these options, HAProxy emerged as the best fit for this project due to its robust features, community support, and long-standing reputation for reliability in production environments.</p>"},{"location":"loadbalancer/#setting-up-haproxy","title":"Setting Up HAProxy","text":"Process Summary <ol> <li>Installing HAProxy via Docker.</li> <li>Configuring <code>haproxy.cfg</code> to define backend servers and load-balancing rules.</li> <li>Testing the configuration locally and ensuring proper traffic distribution.</li> <li>Integrating HAProxy with Flask-based web servers.</li> </ol>"},{"location":"loadbalancer/#getting-started","title":"Getting Started","text":"<p>For a detailed step-by-step guide, refer to the Getting Started page.</p>"},{"location":"loadbalancer/#haproxy-configuration","title":"HAProxy Configuration","text":"<p>Below is a detailed explanation of the <code>haproxy.cfg</code> file used in this project. Each section is broken down with a comparison between the current setup and considerations for a production environment.</p> HAProxy Configuration (<code>haproxy.cfg</code>)globaldefaultsfrontendbackend haproxy.cfg<pre><code>global\n    log stdout format raw local0\n    maxconn 4096\n\ndefaults\n    mode http\n    log     global\n    option  httplog\n    option  dontlognull\n    timeout connect 5s\n    timeout client  10s\n    timeout server  10s\n\nfrontend http_front\n    bind *:60000-60001\n    default_backend backend_servers\n    option forwardfor\n\n\nbackend flask_servers\n    balance roundrobin\n    option httpchk GET /status\n    cookie SERVER insert indirect nocache\n    server web-a web-a:5001 check cookie web-a fall 1 rise 5\n    server web-b web-b:5002 check cookie web-b fall 1 rise 5\n</code></pre> <p><pre><code>global\n    log stdout format raw local0\n    maxconn 4096\n</code></pre> <code>log stdout format raw</code></p> <ul> <li>What it does:<ul> <li>Logs activity to <code>stdout</code> in raw format, useful for debugging in development.</li> </ul> </li> <li>Production Improvement: Redirect logs to a centralized logging solution (e.g., ELK Stack, Splunk) for improved observability and long-term storage.</li> </ul> <p><code>maxconn 4096</code></p> <ul> <li>What it does:<ul> <li>Limits the number of simultaneous connections to 4096.</li> </ul> </li> <li>Production Improvement: Tune this value based on your hardware capabilities and expected traffic; perform benchmarking to find the optimal limit.</li> </ul> <p><pre><code>defaults\n    mode http\n    log     global\n    option  httplog\n    option  dontlognull\n    timeout connect 5s\n    timeout client  10s\n    timeout server  10s\n</code></pre> <code>mode http</code></p> <ul> <li>What it does:<ul> <li>Configures HAProxy to operate in HTTP mode, enabling HTTP-specific functionality.</li> </ul> </li> <li>Production Improvement: Ensure this mode is explicitly set for both frontend and backend sections to avoid ambiguities.</li> </ul> <p><code>log global</code></p> <ul> <li>What it does:<ul> <li>Uses the logging settings defined in the <code>global</code> section.</li> </ul> </li> <li>Production Improvement: Apply log levels (<code>info</code>, <code>warning</code>, etc.) to reduce noise in log output and prioritize critical information.</li> </ul> <p><code>option httplog</code></p> <ul> <li>What it does:<ul> <li>Enables detailed logging of HTTP requests.</li> </ul> </li> <li>Production Improvement: Retain this option and consider adding access log analysis tools for deeper insights into traffic patterns.</li> </ul> <p><code>option dontlognull</code></p> <ul> <li>What it does:<ul> <li>Prevents logging of connections with no data transfer, reducing log noise.</li> </ul> </li> <li>Production Improvement: Keep this option enabled to avoid cluttering logs with irrelevant entries.</li> </ul> <p><code>timeout connect 5s</code></p> <ul> <li>What it does:<ul> <li>Sets a 5-second timeout for establishing connections to backend servers.</li> </ul> </li> <li>Production Improvement: Adjust based on the expected latency and server performance in your environment.</li> </ul> <p><code>timeout client 10s</code></p> <ul> <li>What it does:<ul> <li>Sets a 10-second timeout for receiving data from clients.</li> </ul> </li> <li>Production Improvement: Fine-tune this value to match the behavior of your client applications.</li> </ul> <p><code>timeout server 10s</code></p> <ul> <li>What it does:<ul> <li>Sets a 10-second timeout for responses from backend servers.</li> </ul> </li> <li>Production Improvement: Increase this for resource-intensive operations or slow backend servers.</li> </ul> <p><pre><code>frontend http_front\n    bind *:60000-60001\n    default_backend backend_servers\n    option forwardfor\n</code></pre> <code>bind *:60000-60001</code></p> <ul> <li>What it does:<ul> <li>Listens on ports 60000-60001 for incoming HTTP traffic.</li> </ul> </li> <li>Production Improvement: Use port 443 and SSL certificates for secure HTTPS communication (e.g., <code>bind *:443 ssl crt /path/to/cert.pem</code>).</li> </ul> <p><code>default_backend backend_servers</code></p> <ul> <li>What it does:<ul> <li>Routes all incoming traffic to the <code>backend_servers</code> section.</li> </ul> </li> <li>Production Improvement: Configure separate frontends and backends for different types of traffic, such as APIs or admin interfaces.</li> </ul> <p><code>option forwardfor</code></p> <ul> <li>What it does:<ul> <li>Adds the <code>X-Forwarded-For</code> header to requests, preserving the client IP address.</li> </ul> </li> <li>Production Improvement: Retain this option for proper logging and troubleshooting, especially when using HAProxy as a reverse proxy.</li> </ul> <p><pre><code>backend flask_servers\n    balance roundrobin\n    option httpchk GET /status\n    cookie SERVER insert indirect nocache\n    server web-a web-a:5001 check cookie web-a fall 1 rise 5\n    server web-b web-b:5002 check cookie web-b fall 1 rise 5\n</code></pre> <code>balance roundrobin</code></p> <ul> <li>What it does:<ul> <li>Distributes traffic evenly across backend servers in a round-robin fashion.</li> </ul> </li> <li>Production Improvement: Use algorithms like <code>leastconn</code> (for uneven workloads) or <code>source</code> (to maintain session persistence).</li> </ul> <p><code>option httpchk GET /status</code></p> <ul> <li>What it does:<ul> <li>Sends HTTP GET requests to <code>/status</code> to check the health of backend servers.</li> </ul> </li> <li>Production Improvement: Customize the health check endpoint to include application-level checks (e.g., database connectivity).</li> </ul> <p><code>cookie SERVER insert indirect nocache</code></p> <ul> <li>What it does:<ul> <li>Inserts a session cookie to maintain persistence across requests.</li> </ul> </li> <li>Production Improvement: Ensure secure cookie attributes (<code>Secure</code>, <code>HttpOnly</code>) are set when using HTTPS.</li> </ul> <p><code>server web-a web-a:5001 check cookie web-a fall 1 rise 5</code></p> <ul> <li>What it does:<ul> <li>Defines a backend server (<code>web-a</code>) at <code>web-a:5001</code> with health checks. The server is marked down after 1 failed check (<code>fall</code>) and marked up after 5 successful checks (<code>rise</code>).</li> </ul> </li> <li>Production Improvement: Use FQDNs for server names and tune <code>fall</code> and <code>rise</code> values to reflect real-world conditions and minimize downtime.</li> </ul> <p><code>server web-b web-b:5002 check cookie web-b fall 1 rise 5</code></p> <ul> <li>What it does:<ul> <li>Similar to <code>web-a</code>, defines another backend server (<code>web-b</code>) at <code>web-b:5002</code> with identical settings.</li> </ul> </li> <li>Production Improvement: Add more backend servers or dynamically scale using a service discovery mechanism like Consul or Kubernetes.</li> </ul>"},{"location":"loadbalancer/#recommendations-for-production","title":"Recommendations for Production","text":"Key Improvements for Production <ul> <li>Centralized Logging: Send logs to external monitoring tools like ELK Stack or Splunk for detailed analysis.</li> <li>SSL Termination: Configure SSL certificates to enable secure HTTPS communication.</li> <li>Dynamic Scaling: Integrate with service discovery tools to dynamically update backend servers.</li> <li>Rate Limiting: Add protection against abuse by limiting the number of requests per IP address.</li> <li>Error Handling: Use <code>errorfile</code> directives to customize error responses.</li> </ul>"},{"location":"loadbalancer/#challenges-faced","title":"Challenges Faced","text":"Challenges Encountered <ul> <li>Configuration Complexity: Understanding the nuances of haproxy.cfg and adapting it for the project\u2019s needs.</li> <li>Logging and Monitoring: Ensuring adequate visibility into traffic distribution and server health.</li> <li>Performance Tuning: Optimizing HAProxy settings to handle potential traffic spikes.</li> </ul>"},{"location":"loadbalancer/#how-i-solved-them","title":"How I Solved Them","text":"<ul> <li>Configuration Complexity:<ul> <li>Consulted official HAProxy documentation and online resources.</li> <li>Experimented with different configurations to find the most suitable setup.</li> </ul> </li> <li>Logging and Monitoring:<ul> <li>Integrated logging using the log directive in haproxy.cfg.</li> <li>Enabled health checks for backend servers.</li> </ul> </li> <li>Performance Tuning:<ul> <li>Adjusted maxconn and timeout settings to accommodate varying traffic loads.</li> </ul> </li> </ul>"},{"location":"loadbalancer/#lessons-learned","title":"Lessons Learned","text":"Key Takeaways <ul> <li>HAProxy\u2019s flexibility allows fine-grained control over traffic distribution and server health monitoring.</li> <li>Proper logging and monitoring are essential for diagnosing issues and ensuring high availability.</li> <li>Iterative testing and tuning are critical for achieving optimal performance in production.</li> </ul>"},{"location":"loadbalancer/#articles-and-resources","title":"Articles and Resources","text":"<p>Here are some resources I used to understand and configure HAProxy:</p> <ul> <li>Official HAProxy Documentation</li> <li>Four Essential HAProxy Configurations<ul> <li>Going over: global, defaults, frontend, backend.</li> </ul> </li> <li>Optimizing HAProxy Configuration<ul> <li>Goes over more configuration settings targetted towards High Traffic.</li> </ul> </li> <li>HAProxy Health Checks</li> <li>Community Forum Discussion</li> </ul> <p>Future Enhancements</p> <p>Framework and Application Changes</p> <ul> <li>Explore more advanced load-balancing algorithms in HAProxy.</li> <li>Evaluate the need for SSL termination at the load balancer level.</li> </ul> <p>Monitoring and Security</p> <ul> <li>Enhance monitoring with tools like Prometheus and Grafana for better traffic insights.</li> <li>Implement additional security measures, such as rate limiting and IP whitelisting.</li> </ul>"},{"location":"nagios/","title":"Nagios: Monitoring and Observability","text":"<p>This section provides a Nagios-centric perspective, covering its setup, configuration, challenges, and lessons learned. It includes detailed explanations, configuration examples, and external resources that guided the implementation.</p>"},{"location":"nagios/#overview","title":"Overview","text":"What You'll Learn <p>By the end of this section, you will understand:</p> <ul> <li>Why Nagios was chosen for monitoring.</li> <li>The process of setting up and configuring Nagios.</li> <li>Key challenges faced during the implementation.</li> <li>Lessons learned and resources that guided the implementation.</li> </ul> <p>Nagios plays a critical role in this project by providing monitoring and observability, ensuring system health and quick detection of potential issues to maintain uptime and reliability.</p>"},{"location":"nagios/#choosing-nagios","title":"Choosing Nagios","text":""},{"location":"nagios/#why-nagios","title":"Why Nagios?","text":"Key Benefits of Nagios <ul> <li>Scalability: Easily integrates with large infrastructures.</li> <li>Customizability: Allows tailored checks and plugins.</li> <li>Reliability: Proven for monitoring critical systems over time.</li> <li>Community-Driven: Strong support and vast resources available.</li> </ul> <pre><code>graph TD\n    A[Admin] --&gt; B(Nagios)\n    B --&gt; C[Web Servers]\n    B --&gt; D[LoadBalancer]</code></pre> <p>Due to the purpose of this project and the requirement of using Nagios. This was the only option to choose from. I had heard about Nagios but this was the first time deploying and configuring this monitoring solution.</p>"},{"location":"nagios/#setting-up-nagios","title":"Setting Up Nagios","text":"Process Summary <ol> <li>Installing Nagios Core on a Linux-based system.</li> <li>Configuring nagios.cfg to define hosts, services, and alerting rules.</li> <li>Adding custom plugins for project-specific monitoring needs.</li> <li>Testing configurations and ensuring proper alerting mechanisms.<ul> <li>Due to the nature of the setup we are leveraging Docker Image and Volumes to add custom settings/configurations.</li> </ul> </li> </ol> <p>Nagios Configuration</p> <p>This project uses the Nagios Docker image from Jason Rivers Repository</p> <ul> <li>Volumes are used to add custom configuration files, such as <code>webservers.cfg</code> and <code>haproxy.cfg</code>, to the Docker container. This allows for dynamic and flexible updates to the monitoring setup.</li> </ul> Nagios ConfigurationCommandsHAProxyWeb Servers <p>nagios.cfg<pre><code># Things Changed\ncfg_file=/usr/local/nagios/etc/objects/commands.cfg\ncfg_file=/opt/nagios/etc/objects/webservers.cfg\ncfg_file=/opt/nagios/etc/objects/haproxy.cfg\n</code></pre> <code>/usr/local/nagios/etc/objects/commands.cfg</code></p> <ul> <li>What it does now: Specifies the location of the commands.cfg file, which contains definitions for custom check commands.</li> <li>Production Changes: Ensure this file is regularly reviewed and version-controlled. Consider separating critical commands into distinct directories for modularity.</li> </ul> <p><code>/opt/nagios/etc/objects/webservers.cfg</code> and <code>/opt/nagios/etc/objects/haproxy.cfg</code></p> <ul> <li>What it does now:<ul> <li>Includes configuration files for web servers and the HAProxy load balancer.</li> </ul> </li> <li>Production Changes:<ul> <li>Use structured directories for organizing files (e.g., objects/web/ or objects/load_balancers/).</li> <li>Enforce naming conventions.</li> </ul> </li> </ul> commands.cfg<pre><code># 'check_webservers' custom command definition\ndefine command {\n        command_name    check_webservers\n        command_line    /opt/nagios/libexec/check_webservers.py\n}\n</code></pre> <ul> <li> <p>What it does now:</p> <ul> <li>Defines a custom command check_webservers, which executes the specified Python script <code>check_webservers.py</code>.</li> </ul> </li> <li> <p>Production Changes:</p> <ul> <li>Secure the script's execution with permissions and avoid hardcoding sensitive information. Use logging in <code>check_webservers.py</code> for audit and debugging.</li> <li>Consider migrating commands to a centralized directory under libexec/custom/ for better organization.</li> </ul> </li> </ul> <p>haproxy.cfg<pre><code>define host {\n        use                     linux-server\n        host_name               haproxy\n        alias                   HAProxy Load Balancer\n        address                 haproxy\n}\ndefine hostgroup {\n        hostgroup_name  haproxy-servers      ;\n        alias           HAProxy Servers      ;\n        members         haproxy      ;\n}\ndefine service {\n        use                             local-service,graphed-service\n        host_name                       haproxy\n        service_description             PING\n        check_command                   check_ping!100.0,20%!500.0,60%\n        check_interval                  1    ;\n        retry_interval                  0.5  ;\n        max_check_attempts              3    ;\n}\ndefine service {\n        use                             local-service,graphed-service\n        host_name                       haproxy\n        service_description             Current Load\n        check_command                   check_local_load!5.0,4.0,3.0!10.0,6.0,4.0\n}\n</code></pre> <code>define host</code></p> <ul> <li> <p>What it does now:</p> <ul> <li>Defines the HAProxy host with a specific alias and hostname.</li> </ul> </li> <li> <p>Production Changes:</p> <ul> <li>Use a fixed IP address or DNS entry for stability.</li> <li>Include contact groups to define alerting policies.</li> </ul> </li> </ul> <p><code>define hostgroup</code></p> <ul> <li>What it does now:<ul> <li>Groups all HAProxy servers for unified monitoring.</li> </ul> </li> <li>Production Changes:<ul> <li>Define multiple HAProxy instances if needed for redundancy.</li> <li>Add tagging for roles (e.g., primary, secondary).</li> </ul> </li> </ul> <p><code>define service</code></p> <p>PING</p> <ul> <li>What it does now:<ul> <li>Monitors HAProxy availability via ICMP ping.</li> </ul> </li> <li>Production Changes:<ul> <li>Add notification thresholds for escalation (e.g., email after 3 failures).</li> <li>Consider disabling ICMP in high-security environments and replacing with TCP-based checks.</li> </ul> </li> </ul> <p>Current Load</p> <ul> <li>What it does now:<ul> <li>Checks HAProxy's CPU load against warning and critical thresholds.</li> </ul> </li> <li>Production Changes:<ul> <li>Adjust thresholds based on server capacity and observed trends.</li> <li>Use dynamic load metrics via plugins.</li> </ul> </li> </ul> <p>webservers.cfg<pre><code>define host {\n        use                     linux-server\n        host_name               web-a\n        alias                   Flask Web Server A\n        address                 web-a\n}\ndefine host {\n        use                     linux-server\n        host_name               web-b\n        alias                   Flask Web Server B\n        address                 web-b\n}\ndefine hostgroup {\n        hostgroup_name  web-servers      ;\n        alias           Web Servers      ;\n        members         web-a,web-b      ;\n}\ndefine service {\n        use                             local-service,graphed-service\n        host_name                       web-a,web-b\n        service_description             PING\n        check_command                   check_ping!100.0,20%!500.0,60%\n        check_interval                  1    ;\n        retry_interval                  0.5  ;\n        max_check_attempts              3    ;\n}\ndefine service {\n        use                             local-service,graphed-service\n        host_name                       web-a,web-b\n        service_description             HTTP\n        check_command                   check_http\n}\ndefine service {\n        use                             generic-service,graphed-service\n        host_name                       localhost\n        service_description             Web Server Status\n        check_command                   check_webservers!5\n        check_interval                  1    ;\n        retry_interval                  0.5  ;\n        max_check_attempts              2    ;\n}\n</code></pre> <code>define host</code></p> <ul> <li>What it does now:<ul> <li>Defines a host named <code>web-a</code> that uses the <code>linux-server</code> template.</li> <li>Assigns an alias \"Flask Web Server A\" for easier identification in logs and reports.</li> <li>Resolves <code>web-a</code> as the server address (hostname or IP).</li> </ul> </li> <li>Production Changes:<ul> <li>Ensure <code>web-a</code> is a resolvable DNS entry or static IP.</li> <li>Add metadata fields such as <code>location</code> or <code>contact</code> for operational clarity.</li> </ul> </li> </ul> web-b explanation <ul> <li>Configuration is the same for web-b, the name is all that changes.</li> </ul> <p><code>define hostgroup</code></p> <ul> <li>What it does now:<ul> <li>Groups <code>web-a</code> and <code>web-b</code> under the host group <code>web-servers</code>.</li> <li>Simplifies the application of common checks to both servers.</li> </ul> </li> <li>Production Changes:<ul> <li>Add documentation or labels to distinguish production and staging servers.</li> </ul> </li> </ul> <p><code>define service</code></p> <p>PING</p> <ul> <li>What it does now:<ul> <li>Monitors the availability of <code>web-a</code> and <code>web-b</code> using ICMP ping.</li> </ul> </li> </ul> PING Deep Dive <ul> <li>Arguments for <code>check_ping</code>:<ul> <li><code>100.0,20%</code>: Warning thresholds (latency in ms, packet loss percentage).</li> <li><code>500.0,60%</code>: Critical thresholds (latency in ms, packet loss percentage).</li> </ul> </li> <li>Intervals:<ul> <li><code>check_interval</code>: Runs every 1 minute.</li> <li><code>retry_interval</code>: Retries every 30 seconds on failure.</li> <li><code>max_check_attempts</code>: Alerts after 3 failed attempts.</li> </ul> </li> </ul> <ul> <li>Production Changes:<ul> <li>Replace ICMP with application-specific health checks for restricted environments.</li> <li>Adjust thresholds based on network performance trends.</li> </ul> </li> </ul> <p>HTTP</p> <ul> <li>What it does now:<ul> <li>Checks the availability of HTTP services on <code>web-a</code> and <code>web-b</code>.</li> <li><code>check_http</code>: Ensures the web server is responding with an expected HTTP status.</li> </ul> </li> <li>Production Changes:<ul> <li>Add arguments to <code>check_http</code> for SSL validation or response time thresholds.</li> <li>Monitor additional URLs or endpoints.</li> </ul> </li> </ul> <p>check_webservers</p> <ul> <li>What it does now:<ul> <li>Runs the custom <code>check_webservers</code> command to monitor web server health.</li> </ul> </li> </ul> check_webservers Deep Dive <ul> <li>Argument <code>!5</code>: Checks for up to 5 server connections or conditions.</li> <li>Intervals:<ul> <li><code>check_interval</code>: Checks every 1 minute.</li> <li><code>retry_interval</code>: Retries every 30 seconds on failure.</li> <li><code>max_check_attempts</code>: Alerts after 2 failed attempts.</li> </ul> </li> </ul> <ul> <li>Production Changes:<ul> <li>Improve the custom script to include more metrics, such as response time and error rates.</li> <li>Log results for analysis and troubleshooting.</li> </ul> </li> </ul>"},{"location":"nagios/#getting-started","title":"Getting Started","text":"<p>For a detailed step-by-step guide, refer to the Getting Started page.</p>"},{"location":"nagios/#recommendations-for-production","title":"Recommendations for Production","text":"Key Improvements for Production <ul> <li>Enable redundant monitoring for high availability.</li> <li>Integrate with modern alerting tools like PagerDuty or Slack.</li> <li>Use encryption for communication between Nagios server and monitored hosts.</li> </ul>"},{"location":"nagios/#challenges-faced","title":"Challenges Faced","text":"Challenges Encountered <ul> <li>Configuration Complexity: Understanding and adapting nagios.cfg and plugins for project-specific needs.</li> <li>Alert Noise: Fine-tuning thresholds to reduce false positives.</li> <li>Scalability: Adjusting Nagios to efficiently handle growing infrastructure.</li> </ul>"},{"location":"nagios/#first-time-using-nagios","title":"First time using Nagios","text":"<p>This was very confusing at first, especially when just starting out. Thankfully, the documentation is good, and there are plenty of examples to use as a starting point.</p> <p>I chose the easier route by using a Docker image that includes many boilerplate configurations. While these configurations would need adjustments for production, it works quite well as a starting point.</p>"},{"location":"nagios/#how-i-solved-them","title":"How I Solved Them","text":"<ul> <li>Configuration Complexity:<ul> <li>Consulted official Nagios documentation and community forums.</li> </ul> </li> </ul>"},{"location":"nagios/#lessons-learned","title":"Lessons Learned","text":"Key Takeaways <ul> <li>Nagios\u2019 plugin system provides unparalleled flexibility for custom monitoring.</li> <li>Reducing alert noise is essential for effective monitoring.</li> <li>Distributed monitoring is critical for scaling in large environments.</li> </ul>"},{"location":"nagios/#articles-and-resources","title":"Articles and Resources","text":"<p>Here are some resources I used to understand and configure HAProxy:</p> <ul> <li>Official Nagios Documentation</li> <li>Deploy Nagios as Container</li> <li>Docker-Nagios</li> <li>NRPE Configuration Guide</li> </ul> <p>Future Enhancements</p> <p>Integration and Automation</p> <ul> <li>Explore integration with Prometheus for metrics collection.</li> <li>Automate plugin deployment using Ansible.</li> </ul> <p>Security and Monitoring</p> <ul> <li>Implement SSL/TLS for secure communication.</li> <li>Evaluate advanced visualization tools like Grafana for better insights.</li> </ul>"},{"location":"webserver/","title":"Web Servers: Flask","text":"<p>This section covers the journey of creating web servers for this project using Flask, exploring the lessons learned, challenges faced, and decisions made along the way. Below, you'll find detailed insights into the thought process, tools, and technologies that contributed to building robust and scalable web servers.</p>"},{"location":"webserver/#overview","title":"Overview","text":"What You'll Learn <p>By the end of this section, you will understand:</p> <ul> <li>Why Flask was chosen over alternatives like Nginx or other frameworks.</li> <li>The process of setting up the web servers.</li> <li>Key challenges faced and how they were overcome.</li> <li>Lessons learned and resources that guided the implementation.</li> </ul> <p>I have worked with web servers before in personal projects and tutorials, using frameworks like Flask, Node.js, and Django. This familiarity allowed me to quickly build a barebones Flask app, which can be easily modified into a full-fledged web application.</p>"},{"location":"webserver/#choosing-flask","title":"Choosing Flask","text":""},{"location":"webserver/#why-flask","title":"Why Flask?","text":"Key Benefits of Flask <ul> <li>Lightweight and minimal.</li> <li>Easy to integrate with Python-based projects.</li> <li>Supports rapid development with an extensive plugin ecosystem.</li> </ul> <p>While evaluating frameworks, I considered:</p> <ul> <li>Flask: A micro-framework offering simplicity and flexibility.</li> <li>Nginx: Primarily for reverse proxy and load balancing.</li> <li>Django: A full-stack framework with a heavier footprint.</li> </ul> <p>After comparing these options, Flask emerged as the best fit for this project due to its simplicity and suitability for rapid prototyping.</p> <pre><code>graph TD\n    A[Framework Evaluation] --&gt;|Flexibility| B(Flask)\n    A --&gt;|Scalability| C(Nginx)\n    A --&gt;|Features| D(Django)\n    B --&gt; E[Final Choice]</code></pre> <p>While Django is familiar and full-featured, it was too heavy for this lightweight project. NGINX offered simplicity but lacked the Python-centric flexibility I preferred. Flask\u2019s minimalism and plugin ecosystem made it the ideal choice for rapid prototyping.</p>"},{"location":"webserver/#setting-up-the-web-servers","title":"Setting Up the Web Servers","text":"Process Summary <p>The web server setup involved:</p> <ol> <li>Installing Flask and configuring the application.</li> <li>Setting up routes for API endpoints.</li> <li>Integrating environment variables using <code>python-dotenv</code>.</li> <li>Running the server locally for testing and debugging.</li> </ol> app.py<pre><code>import os\n\nfrom dotenv import load_dotenv\nfrom flask import Flask, jsonify, render_template, request\n\napp = Flask(__name__)\n\nload_dotenv()\n\nweb_env = os.getenv(\"ENV\", \"development\")\nmessage = os.getenv(\"MESSAGE\", \"default\")\n\nif web_env == \"development\":\n    app.config[\"DEBUG\"] = True\nelse:\n    app.config[\"DEBUG\"] = False\n\n\n@app.route(\"/\")\ndef index():\n    visitor_ip = request.remote_addr\n    return render_template(\"index.html\", message=message, visitor_ip=visitor_ip)\n\n\n@app.route(\"/status\", methods=[\"GET\"])\ndef status():\n    return jsonify({\"status\": \"ok\"}), 200\n\n\nif __name__ == \"__main__\":\n    is_production = os.getenv(\"FLASK_ENV\") == \"production\"\n    if is_production:\n        print(\"Running in production mode...\")\n    else:\n        print(\"Running in development mode...\")\n        app.run(host=\"0.0.0.0\", port=5001, debug=app.config[\"DEBUG\"])\n</code></pre>"},{"location":"webserver/#getting-started","title":"Getting Started","text":"<p>For detailed, step-by-step instructions on how to set up and run this application, refer to the Getting Started guide.</p>"},{"location":"webserver/#flask-web-app-talk","title":"Flask Web App Talk","text":"<p>This Flask application demonstrates environment-based configurations, dynamic routing, and basic API functionality. Below, you'll find detailed explanations for each relevant section of the code. <pre><code>graph TD\n    User --&gt;|Access Homepage| Flask[Flask App]\n    User --&gt;|Access /status| Flask[Flask App]\n    Flask --&gt;|Render| HTML[Rendered index.html]\n    Flask --&gt;|Return| JSON[Status: OK]</code></pre></p> Environment ConfigurationRoutesRunning the Application <p>Environment Configuration<pre><code>import os\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env\nload_dotenv()\n\n# Get WEB_ENV from environment\nweb_env = os.getenv(\"ENV\", \"development\")\n\n# Fetch message from environment variables\nmessage = os.getenv(\"MESSAGE\", \"default\")\n\n# Configure Flask based on the environment\nif web_env == \"development\":\n    app.config[\"DEBUG\"] = True\nelse:\n    app.config[\"DEBUG\"] = False\n</code></pre> This section:</p> <ul> <li>Loads environment variables using <code>python-dotenv</code>.</li> <li>Configures <code>DEBUG</code> mode dynamically based on the <code>ENV</code> variable.</li> <li>Fetches a custom message (<code>MESSAGE</code>) from environment variables.</li> </ul> <p>Routes<pre><code>from flask import Flask, jsonify, render_template, request\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef index():\n    # Get the visitor's IP address\n    visitor_ip = request.remote_addr\n    return render_template(\"index.html\", message=message, visitor_ip=visitor_ip)\n\n@app.route(\"/status\", methods=[\"GET\"])\ndef status():\n    return jsonify({\"status\": \"ok\"}), 200\n</code></pre> This section defines two routes:</p> <ul> <li>Homepage (<code>/</code>): Displays an HTML page (<code>index.html</code>) with:<ul> <li>A message fetched from environment variables.</li> <li>The visitor's IP address.</li> </ul> </li> <li>Health Check (<code>/status</code>): Returns a JSON response <code>{\"status\": \"ok\"}</code> with status code <code>200</code>.</li> </ul> <p>Running the Application<pre><code>if __name__ == \"__main__\":\n    # Check the environment to determine the server to use\n    is_production = os.getenv(\"FLASK_ENV\") == \"production\"\n    if is_production:\n        # Use Gunicorn for production\n        print(\"Running in production mode...\")\n    else:\n        # Use Flask's development server\n        print(\"Running in development mode...\")\n        app.run(host=\"0.0.0.0\", port=5001, debug=app.config[\"DEBUG\"])\n</code></pre> This section:</p> <ul> <li>Determines if the application is running in production or development mode using the <code>FLASK_ENV</code> variable.</li> <li>Prints the mode to the console for debugging purposes.</li> <li>Runs the Flask development server on port <code>5001</code> by default.</li> </ul>"},{"location":"webserver/#challenges-faced","title":"Challenges Faced","text":"Challenges Encountered <ul> <li>Deployment Issues: Configuring Flask to work efficiently in a production environment.</li> <li>Learning Curve: Adjusting to Flask's minimalistic approach compared to full-stack frameworks.</li> </ul> <p>How I Solved Them</p> <ul> <li>Deployment: Leveraged Gunicorn for production readiness. (via Docker)</li> <li>Scalability: Added Haproxy as a reverse proxy for better load balancing.</li> <li>Learning Curve: Explored tutorials, Flask documentation, and online communities</li> <li>Flask Documentation: flask.palletsprojects.com</li> <li>Python Dotenv: dotenv documentation</li> </ul>"},{"location":"webserver/#lessons-learned","title":"Lessons Learned","text":"Key Takeaways <ul> <li>Start small and scale incrementally.</li> <li>Flask is great for rapid prototyping but needs additional tools (e.g., Gunicorn, Nginx) for production.</li> <li>Document every step\u2014debugging becomes easier.</li> </ul> <p>The primary goal of this project was to create a simple yet effective way to launch web servers with load balancing (LB) and monitoring tools. This setup serves as a solid starting point, with independent components that can be expanded or replaced as needed. For instance, Flask operates as a standalone service, making it easy to swap out for frameworks like Django or FastAPI if the project requirements evolve.</p> <p>This documentation is written not only to share my project but also to offer insights into my approach, acknowledging that there are multiple ways to solve the same problem. My implementation reflects the choices I made based on my understanding and preferences, but it\u2019s not necessarily the \"perfect\" or \"best\" way to do it. I encourage readers to adapt, improve, or reimagine these ideas to suit their own needs.</p> <p>What\u2019s Next?</p> <p>Framework and Application Changes</p> <ul> <li>Exploring alternatives like FastAPI for performance optimization.</li> <li>If the application becomes a bigger project, consider transitioning to Django.</li> <li>If the frontend grows, introduce frameworks like Bootstrap or other frontend libraries.</li> </ul> <p>Storage and Scalability</p> <ul> <li>No storage currently exists; implement a database or storage solution based on the use case.</li> <li>Scaling the web server architecture to handle high traffic effectively.</li> </ul> <p>Monitoring and Security</p> <ul> <li>Add detailed logging and monitoring tools to track application performance.</li> <li>Introduce security measures, such as HTTPS, input validation, and CSRF protection.</li> </ul>"}]}